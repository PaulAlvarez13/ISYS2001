{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "twitter_data_cleaning.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaulAlvarez13/ISYS2001/blob/main/twitter_data_cleaning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Twitter\n",
        "\n",
        "In this notebook we are going to extract some twitter posts.\n",
        "\n",
        "# snscrape\n",
        "\n",
        "This software is a scraper for social networking services (SNS). It scrapes things like user profiles, hashtags. The following services are currently supported:\n",
        "\n",
        "* Facebook: user profiles, groups, and communities (aka visitor posts)\n",
        "* Instagram: user profiles, hashtags, and locations\n",
        "* Mastodon: user profiles and toots (single or thread)\n",
        "* Reddit: users, subreddits, and searches (via Pushshift)\n",
        "* Telegram: channels\n",
        "* **Twitter: users, user profiles, hashtags, searches, tweets (single or surrounding thread), list posts, and trends**\n",
        "* VKontakte: user profiles\n",
        "* Weibo (Sina Weibo): user profiles\n",
        "\n",
        "We are going to use [snscrape](https://github.com/JustAnotherArchivist/snscrape) to get twitter data. Some of the benefits of using this package are:\n",
        "\n",
        "- Can fetch almost all Tweets \n",
        "- Fast initial setup;\n",
        "- Can be used anonymously and without Twitter sign up;\n",
        "- No rate limitations (don't abuse).\n",
        "\n",
        "> This is not a tutorial on snscrape.  We will use the tool in a simple way.  You can search for documentation and example on the internet is you want to take further.\n",
        "\n",
        "\n",
        "There are alternative.  Some of the more common options include:\n",
        "\n",
        "* [**Tweepy**](https://www.tweepy.org/). Use your developer account credintals to start scraping. Tweepy has a scraping limit of 3200 tweets and seven day history.\n",
        "> I am tired of having to `create and account`, so although simple to use I have decided to try a different package\n",
        "\n",
        "* [**Twitter’s Firehose API**](https://developer.twitter.com/en/docs/twitter-api/enterprise/compliance-firehose-api/overview) You can upgrade you developer account and get nearly unlimited access to Twitter’s data stream via one of the various Twitter data provider partners.\n",
        "\n",
        "* [**GetOldTweets3**](https://pypi.org/project/GetOldTweets3/) Twitter has removed the endpoint the GetOldTweets3 uses and that makes GOT no longer useful. Included because it may come up in conversation.\n",
        "\n",
        "* [**TWINT**](https://github.com/twintproject/twint). Twint (Twitter Intelligence Tool) is an advanced tool, currently difficult to install. The author of the library recommends using a Dockerfile which is difficult on Colab.\n",
        "\n",
        "* [**Octoparse**](https://www.octoparse.com/) Octoparse is a paid software with free option with limitations.\n",
        "\n",
        "\n",
        "* **Use existing data**\n",
        "  * [Top 25 Twitter Datasets for Natural Language Processing and Machine Learning](https://imerit.net/blog/top-25-twitter-datasets-for-natural-language-processing-and-machine-learning-all-pbm/)\n",
        "  * [Free Twitter Datasets Mega Compilation](https://www.trackmyhashtag.com/blog/free-twitter-datasets/)\n",
        "  * [Awesome Twitter Data](https://awesomeopensource.com/project/shaypal5/awesome-twitter-data)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "osxDcdYiJp6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the pip install command below if you don't already have the library\n",
        "!pip install snscrape"
      ],
      "metadata": {
        "id": "mQHsIdp2zDZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data\n",
        "\n",
        "We will follow our typical pattern\n",
        "* import the libraries\n",
        "* setup the variable(s)\n",
        "* do the calculation\n",
        "* output the results\n",
        "\n",
        "We will use a common strategy of storing the data into a Pandas dataframe.\n",
        "\n",
        "\n",
        "## The Query\n",
        "\n",
        "As the purpose of the notebook is to visualise data.  We will keep our query simple\n",
        "\n",
        "*How often is `Python` mentioned in tweets.  We will limit our scrape to a modest 500 tweets between `January 1, 2022`, and `May 1, 2022`.*\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HcgyD_Sta-KR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import snscrape.modules.twitter as sntwitter\n",
        "import pandas as pd\n",
        "\n",
        "# Setting variables to be used below\n",
        "maxTweets = 50\n",
        "\n",
        "# Creating list to append tweet data to\n",
        "tweets = []\n",
        "\n",
        "# Using TwitterSearchScraper to scrape data and append tweets to list\n",
        "for i, tweet in enumerate(sntwitter.TwitterSearchScraper('python since:2022-01-01 until:2022-05-01').get_items()):\n",
        "    if i > maxTweets:\n",
        "        break\n",
        "    tweets.append([tweet.date, tweet.id, tweet.content])\n",
        "\n",
        "# Creating a dataframe from the tweets list above\n",
        "tweets_df = pd.DataFrame(tweets, columns=['Datetime', 'Tweet Id', 'Text'])\n",
        "\n",
        "# Display first 5 entries from dataframe\n",
        "tweets_df.head()"
      ],
      "metadata": {
        "id": "KIBJGZm3SfJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The text of the tweet contains non-english words, emojis, and strange puncutation.  Depending on you business need these may be important.  For this exercise we will focus on the english words.\n",
        "\n",
        "> For the curious, here is a pythonic way using list generators.  We havent discussed list generators so have followed our pattern of creating an empty list, and then appending to the list.\n",
        "```python\n",
        "query = sntwitter.TwitterSearchScraper('python since:2022-01-01 until:2022-05-01').get_items()\n",
        "tweet_data = [next(query) for _ in range(maxTweets)]\n",
        "tweets = [[tweet.date, tweet.id, tweet.content] for tweet in tweet_data ]\n",
        "```\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hgYeIF5OZAzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clean the data\n",
        "\n",
        "Real world data is *messy*.  In the next cell run some python code to *clean* the data.  In our case this means extracting the english words.\n",
        "\n",
        "We are going to use a [Regular Expression](https://en.wikipedia.org/wiki/Regular_expression).  A regulare expression is a way to use a sequence of characters as a search pattern.  We can extract substrings that match the pattern.  This is not 'perfect' but will meets our need of the exercise."
      ],
      "metadata": {
        "id": "TO9Bt2VSYhSl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean(text):\n",
        "  ''' Uses regular expresison to extract english letter and digits from the supplied text. '''\n",
        "  regExp = \"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\"\n",
        "  return ' '.join(re.sub(regExp, \" \", text).split())\n",
        "\n",
        "tweets_df['Clean Text'] = tweets_df['Text'].apply(clean)\n",
        "tweets_df.head()"
      ],
      "metadata": {
        "id": "bxY8R1p3Ygc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save the data\n",
        "\n",
        "For this exercise we only want the date and the cleaned data.  Once extracted save to csv file."
      ],
      "metadata": {
        "id": "1PCKBzYanQlo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_df['Date'] = pd.to_datetime(tweets_df['Datetime']).dt.date\n",
        "tweets_df[['Date','Clean Text']].to_csv(\"tweets.csv\")"
      ],
      "metadata": {
        "id": "-ncv8nKLoZHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "CX1DHs50pBWO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}